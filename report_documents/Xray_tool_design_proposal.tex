% This is samplepaper.tex, a sample chapter demonstrating the
% IAMOT 2024 template, based on the LLNCS macro package for
%  Springer Computer Science proceedings;
% Version 1.0 of 2023/09/10
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{hyperref}


% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Design Proposal: X-ray tool to assist patients in rehabilitating fractures}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Hargun Mujral, Dev Shah, Sharanya Gupta}
\institute{Wat.ai Design Project}
\maketitle              % typeset the header of the contribution

\section{Overview of the problem}
Reading and understanding x-rays is a difficult process that often requires a medical professional to be in the loop. To get more specific insights regarding certain breakages or injuries often requires multiple sets of eyes, and the waiting times to see a specialist can vary from hours to months. 
\\

\noindent Thus, having some form of automation in this process, to some reasonable degree of accuracy, can benefit both patients and clinicians alike. 

\section{Solution}

\noindent We propose a 3-step architecture that uses computer vision, Generative AI and image classification to help solve this problem.\\

\textbf{Step one:} A binary (or multiary) classifier that tells the user whether a given xray image contains a fracture, ligament tear or some other form of damage. If the answer is no, then we stop and return to the user that no issues have been found. If the answer is yes, then we would return the specific damage type, and move onto the next step. This model would need to be trained on an xray dataset of various limbs, with manual labelling having been done. We would start with a good classifier model, and work from there. \\

\textbf{Step two:}  An image recognition or xray-specific engine that figures out what body part is hurting. This may require a regular image of the body, with a circle around the area in question. Or, if there exists some funcitonal dataset that works directly on xrays, we would use that instead. This part requires computer vision, to be able to box the smaller area in the full picture, and correctly label it. \\

\textbf{Step three:}  A natural language system, using vector store and/or document embeddings to give the user relevant information regarding what they can do to heal the damage. We would fill a vector db with documents that are relevant, and embed them so we can query for relevant information with langchain and a GPT-style LLM. This way we can ensure accurate information for the patient, and automate the pipeline as much as possible.\\

\noindent Between each step, the user will be involved in the feedback loop. That way, if one of the 3 models makes some sort of error, the user can correct it to the best of their knowledge. For example, if the first model says no fracture, the user can override it and say “ligament tear”, and move onto the next step. If the second model misidentifies the body part, the user can manually select what they think is the correct choice.\\

\noindent At the end, this system should be built to work without the use of AI (the user can manually lookup any information needed with our platform), but the AI’s purpose is to make it easier for users to interact with. This would not be a replacement for seeing a specialist, but instead an aid to help patients get an idea of how to rehabilitate when they do not have access to medical professionals right away.

\section{Relevant Datasets}

Bone fractures dataset FracAtlas (4000+ datapoints): 
\href{https://figshare.com/articles/dataset/The_dataset/22363012}{FracAtlas}\\

\noindent X-ray body part recognition (2000+ datapoints): \href{https://www.kaggle.com/competitions/unifesp-x-ray-body-part-classifier}{UNIFESP}\\

\noindent We will need to continue to do research to find relevant datasets

\section{Timeline \& Details}

We anticipate that this would be a long process if we do not make concrete goals and have motivated members. Thus one of our selection criteria would be motivated members who have enough time to work on the project. \\

\noindent Building out each of the three models would require some collaboration, but in essence are separate enough that we can have different members working on different models. Due to the workload requirements, this would likely be a bigger team with an extra TPM and/or core member. \\


\noindent Here is the monthly timeline for Fall 2023:\\
\begin{itemize} 
    \item By October's end: we want to ensure we have good data, and want to preprocess it so that we can begin development for the first two models. We will research different models and create an action plan for developing the image classifier and computer vision systems. \\
    \item By November's end: We will set up the framework for our vector database, langchain, and GPT model. We will begin development on the two image models, with concrete goals and tasks\\
    \item By December's end: We will have a basic working prototype, integrated end-to-end. Even if we have abysmal accuracy, the goal is to get the workflow set up by the end of this term\\
\end{itemize}

\noindent It may not be super valuable to speculate beyond this point, however from here we can begin to refine our models, integrate it into an end-to-end website platform and talk to people in the industry to spread awareness of our tool. This would likely be the focus in Winter 2024.


% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%

\end{document}
