{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6696884,"sourceType":"datasetVersion","datasetId":3860653}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Importing the required libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport optuna\nimport os\nimport json\nimport cv2\nimport json\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport torch.optim as optim\nimport torchvision.models.detection\nfrom torchvision.ops import box_iou\nfrom PIL import Image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Now we are extracting the annotations relevant to the one image, extracting bounding box information and corresponding labels.\n* It will display the Image with bounding box using the respective annotation and corresponding labels","metadata":{}},{"cell_type":"code","source":"annotations_path = '/kaggle/input/fracatlas/FracAtlas/Annotations/COCO JSON/COCO_fracture_masks.json'\nimage_directory = '/kaggle/input/fracatlas/FracAtlas/images/Fractured'\n\nwith open(annotations_path) as file:\n    coco_data = json.load(file)\n\ncategory_mapping = {category['id']: category['name'] for category in coco_data['categories']}\n\nimage_info = coco_data['images'][4] \nimage_path = os.path.join(image_directory, image_info['file_name'])\nimage = cv2.imread(image_path)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nannotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == image_info['id']]\n\nplt.imshow(image)\nfor ann in annotations:\n    bbox = ann['bbox']\n    label = category_mapping[ann['category_id']]\n    x, y, w, h = bbox\n    rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n    plt.gca().add_patch(rect)\n    plt.text(x, y, label, color='white', fontsize=8, backgroundcolor='red')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This Class is essentially a data preprocessing of fracatlas images.\nIt takes COCO JSON Annotations and Images as a input for the Data Preprocessing\nThere are three main functions in this class\n1. Getitem -  It retrieves a annotations and calculates the bounding boxes for each image\n2. Len - It returns the number of images in the dataset\n3. Transform - It converts the images to tensors\n","metadata":{}},{"cell_type":"code","source":"class FracAtlasDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transforms=None):\n        self.transforms = transforms\n        with open(annotations_file) as f:\n            self.coco_data = json.load(f)\n        self.img_dir = img_dir\n        self.images = self.coco_data['images']\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.images[idx]['file_name'])\n        img = Image.open(img_path).convert(\"RGB\")\n        num_objs = len(self.coco_data['annotations'])\n        boxes = []\n        for i in range(num_objs):\n            if self.coco_data['annotations'][i]['image_id'] == self.images[idx]['id']:\n                xmin = self.coco_data['annotations'][i]['bbox'][0]\n                ymin = self.coco_data['annotations'][i]['bbox'][1]\n                xmax = xmin + self.coco_data['annotations'][i]['bbox'][2]\n                ymax = ymin + self.coco_data['annotations'][i]['bbox'][3]\n                boxes.append([xmin, ymin, xmax, ymax])\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        if self.transforms is not None:\n            img = self.transforms(img)\n        return img, target\n    def __len__(self):\n        return len(self.images)\ndef get_transform():\n    return torchvision.transforms.Compose([\n        torchvision.transforms.ToTensor(),\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-01-26T02:45:48.077640Z","iopub.execute_input":"2024-01-26T02:45:48.077949Z","iopub.status.idle":"2024-01-26T02:45:48.089943Z","shell.execute_reply.started":"2024-01-26T02:45:48.077926Z","shell.execute_reply":"2024-01-26T02:45:48.089017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* This objective function is the hyperparameter tuning of the Faster RCNN model using Python Optuna (a hyperparameter framework).\n* The function takes a trial object from Optuna and a list to store results. \n* It suggests values for learning rate (lr), weight decay, and number of epochs within specified ranges.\n* This model is optimized using SGD (Stochastic Gradient Descent) After training the model, it is evaluated using Evaluate function and it will result IOU \n* IOU - Intersection Over Union - A common metric in object detection task - this will give the score by comparing the Ground truth bounding boxes with the predicted bounding boxes from the model\n* If the IOU more than 0.5, the Prediction of bounding boxes will be more precise and reliable","metadata":{}},{"cell_type":"code","source":"def objective(trial,results_list):\n    lr = trial.suggest_loguniform('lr', 0.0001,0.001)\n    weight_decay = trial.suggest_uniform('weight_decay', 0.0001,0.001)\n    num_epochs = trial.suggest_int('num_epochs', 2,6)\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    num_classes = 2 \n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    annotations_file = '/kaggle/input/fracatlas/FracAtlas/Annotations/COCO JSON/COCO_fracture_masks.json'\n    img_dir = '/kaggle/input/fracatlas/FracAtlas/images/Fractured'\n    dataset = FracAtlasDataset(annotations_file, img_dir, get_transform())\n    data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    for epoch in range(num_epochs):\n        model.train()\n        for images, targets in data_loader:\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            optimizer.zero_grad()\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            losses.backward()\n            optimizer.step()\n    mean_iou = evaluate_model(model, data_loader, device)\n    trial_result = {\n        'trial_number': trial.number,\n        'mean_iou': mean_iou,\n        'lr': lr,\n        'weight_decay': weight_decay,\n        'num_epochs': num_epochs\n    }\n    results_list.append(trial_result)\n    return mean_iou","metadata":{"execution":{"iopub.status.busy":"2024-01-26T02:45:48.092466Z","iopub.execute_input":"2024-01-26T02:45:48.092752Z","iopub.status.idle":"2024-01-26T02:45:48.106420Z","shell.execute_reply.started":"2024-01-26T02:45:48.092727Z","shell.execute_reply":"2024-01-26T02:45:48.105385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, data_loader, device):\n    model.eval()\n    ious = []\n    with torch.no_grad():\n        for images, targets in data_loader:\n            images = list(img.to(device) for img in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            predictions = model(images)\n\n            for target, prediction in zip(targets, predictions):\n                gt_boxes = target['boxes']\n                pred_boxes = prediction['boxes']\n\n                if pred_boxes.shape[0] > 0 and gt_boxes.shape[0] > 0:\n                    iou = box_iou(gt_boxes, pred_boxes)\n                    ious.extend(iou.flatten().tolist())\n\n    mean_iou = sum(ious) / len(ious) if ious else 0\n    print(mean_iou)\n    return mean_iou","metadata":{"execution":{"iopub.status.busy":"2024-01-26T02:45:48.107598Z","iopub.execute_input":"2024-01-26T02:45:48.107905Z","iopub.status.idle":"2024-01-26T02:45:48.120233Z","shell.execute_reply.started":"2024-01-26T02:45:48.107857Z","shell.execute_reply":"2024-01-26T02:45:48.119313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* I have set the number of trials to 20, the optuna library will produce new hyperparam based on the previous results.\n* Storing the results in the CSV","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(direction='maximize')\ntrial_results = []\nstudy.optimize(lambda trial: objective(trial, trial_results), n_trials=20)\nresults_df = pd.DataFrame(trial_results)\nprint(results_df)\nresults_df.to_csv('optuna_trial_results.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T02:45:48.121429Z","iopub.execute_input":"2024-01-26T02:45:48.121713Z","iopub.status.idle":"2024-01-26T05:31:10.498686Z","shell.execute_reply.started":"2024-01-26T02:45:48.121689Z","shell.execute_reply":"2024-01-26T05:31:10.497829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value (Mean IoU): \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:31:10.500106Z","iopub.execute_input":"2024-01-26T05:31:10.500470Z","iopub.status.idle":"2024-01-26T05:31:10.506571Z","shell.execute_reply.started":"2024-01-26T05:31:10.500438Z","shell.execute_reply":"2024-01-26T05:31:10.505702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_lr = trial.params['lr']\nprint(best_lr)\nbest_weight_decay = trial.params['weight_decay']\nprint(best_weight_decay)\nbest_num_epochs = trial.params['num_epochs']\nprint(best_num_epochs)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:32:42.109856Z","iopub.execute_input":"2024-01-26T05:32:42.110803Z","iopub.status.idle":"2024-01-26T05:32:42.116131Z","shell.execute_reply.started":"2024-01-26T05:32:42.110765Z","shell.execute_reply":"2024-01-26T05:32:42.115143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I had used the best hyperparameters to train the model and saved the model","metadata":{}},{"cell_type":"code","source":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nnum_classes = 2\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\noptimizer = optim.SGD(model.parameters(), lr=best_lr, momentum=0.9, weight_decay=best_weight_decay)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:32:48.132361Z","iopub.execute_input":"2024-01-26T05:32:48.133126Z","iopub.status.idle":"2024-01-26T05:32:48.825715Z","shell.execute_reply.started":"2024-01-26T05:32:48.133091Z","shell.execute_reply":"2024-01-26T05:32:48.824776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_file = '/kaggle/input/fracatlas/FracAtlas/Annotations/COCO JSON/COCO_fracture_masks.json'\nimg_dir = '/kaggle/input/fracatlas/FracAtlas/images/Fractured'\ndataset = FracAtlasDataset(annotations_file, img_dir, get_transform())\ndata_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:33:25.406163Z","iopub.execute_input":"2024-01-26T05:33:25.406521Z","iopub.status.idle":"2024-01-26T05:33:25.429100Z","shell.execute_reply.started":"2024-01-26T05:33:25.406496Z","shell.execute_reply":"2024-01-26T05:33:25.428228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(best_num_epochs):\n    model.train()\n    for images, targets in data_loader:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        optimizer.zero_grad()\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        losses.backward()\n        optimizer.step()\ntorch.save(model.state_dict(), 'best_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-01-26T05:33:26.644349Z","iopub.execute_input":"2024-01-26T05:33:26.644694Z","iopub.status.idle":"2024-01-26T05:41:40.278246Z","shell.execute_reply.started":"2024-01-26T05:33:26.644667Z","shell.execute_reply":"2024-01-26T05:41:40.277185Z"},"trusted":true},"execution_count":null,"outputs":[]}]}