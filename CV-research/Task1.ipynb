{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image, ImageFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets\\FracAtlas\\dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>hand</th>\n",
       "      <th>leg</th>\n",
       "      <th>hip</th>\n",
       "      <th>shoulder</th>\n",
       "      <th>mixed</th>\n",
       "      <th>hardware</th>\n",
       "      <th>multiscan</th>\n",
       "      <th>fractured</th>\n",
       "      <th>fracture_count</th>\n",
       "      <th>frontal</th>\n",
       "      <th>lateral</th>\n",
       "      <th>oblique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG0000000.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG0000001.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  hand  leg  hip  shoulder  mixed  hardware  multiscan  \\\n",
       "0  IMG0000000.jpg     0    1    0         0      0         0          1   \n",
       "1  IMG0000001.jpg     0    1    0         0      0         0          1   \n",
       "\n",
       "   fractured  fracture_count  frontal  lateral  oblique  \n",
       "0          0               0        1        1        0  \n",
       "1          0               0        1        1        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets\\\\FracAtlas\\\\images\\\\Non_fractured'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = 'datasets\\FracAtlas\\images'\n",
    "fractured_path = os.path.join(base_path, 'Fractured')\n",
    "non_fractured_path = os.path.join(base_path, 'Non_fractured')\n",
    "non_fractured_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = {img: os.path.join(fractured_path, img) for img in os.listdir(fractured_path)}\n",
    "image_paths.update({img: os.path.join(non_fractured_path, img) for img in os.listdir(non_fractured_path)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = 'datasets\\FracAtlas\\images'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes PIL ignore bytes beyond the truncation point\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Custom loader that uses PIL and ignores errors in truncated images\n",
    "def custom_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "        \n",
    "dataset = datasets.ImageFolder(root=data_directory, transform=transform, loader=custom_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3266"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "817"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(dataset) - train_size\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x17a179e4dc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x17a179e4ee0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size= 32, shuffle= True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning func "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_model(model, num_classes=2):\n",
    "    if hasattr(model, 'fc'):  # ResNet\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    elif hasattr(model, 'classifier'):  # DenseNet\n",
    "        if isinstance(model.classifier, nn.Sequential):\n",
    "            *layers, last_layer = model.classifier.children()\n",
    "            num_ftrs = last_layer.in_features\n",
    "            new_last_layer = nn.Linear(num_ftrs, num_classes)\n",
    "            model.classifier = nn.Sequential(*layers, new_last_layer)\n",
    "        else:\n",
    "            num_ftrs = model.classifier.in_features\n",
    "            model.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "    else:\n",
    "        raise Exception(\"Unknown model architecture\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=1):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_accuracy = total_correct / len(train_dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / len(val_dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {total_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = 'models'\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training efficientnet_b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gurup\\anaconda3\\envs\\wat\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gurup\\anaconda3\\envs\\wat\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "models_to_try = [models.efficientnet_b0, models.resnet18, models.densenet121]\n",
    "\n",
    "for base_model_func in models_to_try:\n",
    "    print(f\"Training {base_model_func.__name__}\")\n",
    "    base_model = base_model_func(pretrained=True)\n",
    "    model = modify_model(base_model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if hasattr(model, 'fc'):\n",
    "        optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "    elif hasattr(model, 'classifier'):\n",
    "        optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "    else:\n",
    "        raise Exception(\"Unknown model architecture\")\n",
    "    trained_model = train_model(model, criterion, optimizer)\n",
    "    # Save the model\n",
    "    model_path = os.path.join(model_save_dir, f\"{base_model_func.__name__}_model.pth\")\n",
    "    torch.save(trained_model.state_dict(), model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
